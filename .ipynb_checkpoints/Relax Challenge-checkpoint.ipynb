{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85bd908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import countDistinct, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a704fe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/27 22:05:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"user_engagement\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449ed108",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = spark.read.format('csv').options(header='true', inferSchema='true').load(\"takehome_users.csv\")\n",
    "df_engagement = spark.read.format('csv').options(header='true', inferSchema='true').load(\"takehome_user_engagement.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d46966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.createOrReplaceTempView(\"users\")\n",
    "df_engagement.createOrReplaceTempView(\"engagement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "493a1ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff_days = spark.sql(\"\"\"\n",
    "    SELECT user_id, \n",
    "           time_stamp, \n",
    "           datediff(\n",
    "               lead(time_stamp, 1) OVER(PARTITION BY user_id ORDER BY time_stamp), \n",
    "               time_stamp\n",
    "           ) as diff_days \n",
    "    FROM engagement\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "958fe520",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff_days.createOrReplaceTempView(\"diff_days\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d392a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adopted_users = spark.sql(\"\"\"\n",
    "    SELECT user_id \n",
    "    FROM (\n",
    "        SELECT user_id,\n",
    "               sum(diff_days) OVER (\n",
    "                   PARTITION BY user_id \n",
    "                   ORDER BY time_stamp \n",
    "                   ROWS BETWEEN CURRENT ROW AND 2 FOLLOWING\n",
    "               ) as sum_days\n",
    "        FROM diff_days\n",
    "    )\n",
    "    WHERE sum_days <= 7\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3df78398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------+\n",
      "|user_id|         time_stamp|diff_days|\n",
      "+-------+-------------------+---------+\n",
      "|      1|2014-04-22 03:53:30|     null|\n",
      "|      2|2013-11-15 03:45:04|       14|\n",
      "|      2|2013-11-29 03:45:04|       10|\n",
      "|      2|2013-12-09 03:45:04|       16|\n",
      "|      2|2013-12-25 03:45:04|        6|\n",
      "|      2|2013-12-31 03:45:04|        8|\n",
      "|      2|2014-01-08 03:45:04|       26|\n",
      "|      2|2014-02-03 03:45:04|        5|\n",
      "|      2|2014-02-08 03:45:04|        1|\n",
      "|      2|2014-02-09 03:45:04|        4|\n",
      "|      2|2014-02-13 03:45:04|        3|\n",
      "|      2|2014-02-16 03:45:04|       21|\n",
      "|      2|2014-03-09 03:45:04|        4|\n",
      "|      2|2014-03-13 03:45:04|       18|\n",
      "|      2|2014-03-31 03:45:04|     null|\n",
      "|      3|2013-03-19 23:14:52|     null|\n",
      "|      4|2013-05-22 08:09:28|     null|\n",
      "|      5|2013-01-22 10:14:20|     null|\n",
      "|      6|2013-12-19 03:37:06|     null|\n",
      "|      7|2012-12-20 13:24:32|     null|\n",
      "+-------+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_diff_days.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee5d4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adopted_users.createOrReplaceTempView(\"adopted_users\")\n",
    "\n",
    "df_users = spark.sql(\"\"\"\n",
    "    SELECT users.*,\n",
    "           CASE WHEN adopted_users.user_id IS NOT NULL THEN TRUE ELSE FALSE END as is_adopted\n",
    "    FROM users\n",
    "    LEFT JOIN adopted_users ON users.object_id = adopted_users.user_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afb36bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- object_id: integer (nullable = true)\n",
      " |-- creation_time: timestamp (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- creation_source: string (nullable = true)\n",
      " |-- last_session_creation_time: integer (nullable = true)\n",
      " |-- opted_in_to_mailing_list: integer (nullable = true)\n",
      " |-- enabled_for_marketing_drip: integer (nullable = true)\n",
      " |-- org_id: integer (nullable = true)\n",
      " |-- invited_by_user_id: integer (nullable = true)\n",
      " |-- is_adopted: boolean (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93560713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|is_adopted|\n",
      "+----------+\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|      true|\n",
      "|      true|\n",
      "|      true|\n",
      "|      true|\n",
      "|      true|\n",
      "|      true|\n",
      "|      true|\n",
      "|      true|\n",
      "|      true|\n",
      "|      true|\n",
      "|      true|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_users.select(\"is_adopted\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53fafd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3930fa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_dummies = pd.get_dummies(df_users, columns=['creation_source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd05a257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataFrame[object_id: int, creation_time: timestamp, name: string, email: string, creation_source: string, last_session_creation_time: int, opted_in_to_mailing_list: int, enabled_for_marketing_drip: int, org_id: int, invited_by_user_id: int, is_adopted: boolean]]\n"
     ]
    }
   ],
   "source": [
    "print(list(df_user_dummies.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c176fef5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['is_adopted'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf_user_dummies\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_adopted\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m df_user_dummies[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_adopted\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Split data into training and test sets\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5262\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5405\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5406\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['is_adopted'] not found in axis\""
     ]
    }
   ],
   "source": [
    "X = df_user_dummies.drop(\"is_adopted\", axis=1)\n",
    "y = df_user_dummies[\"is_adopted\"]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0093496",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `features` cannot be resolved. Did you mean one of the following? [`is_adopted`, `users`.`name`, `users`.`email`, `users`.`org_id`, `users`.`object_id`].;\n'Project ['features, is_adopted#83]\n+- Project [object_id#17, creation_time#18, name#19, email#20, creation_source#21, last_session_creation_time#22, opted_in_to_mailing_list#23, enabled_for_marketing_drip#24, org_id#25, invited_by_user_id#26, is_adopted#83, creation_source_index#219, UDF(cast(creation_source_index#219 as double), 0) AS creation_source_encoded#239]\n   +- Project [object_id#17, creation_time#18, name#19, email#20, creation_source#21, last_session_creation_time#22, opted_in_to_mailing_list#23, enabled_for_marketing_drip#24, org_id#25, invited_by_user_id#26, is_adopted#83, UDF(cast(creation_source#21 as string)) AS creation_source_index#219]\n      +- Project [object_id#17, creation_time#18, name#19, email#20, creation_source#21, last_session_creation_time#22, opted_in_to_mailing_list#23, enabled_for_marketing_drip#24, org_id#25, invited_by_user_id#26, CASE WHEN isnotnull(user_id#55) THEN true ELSE false END AS is_adopted#83]\n         +- Join LeftOuter, (object_id#17 = user_id#55)\n            :- SubqueryAlias users\n            :  +- View (`users`, [object_id#17,creation_time#18,name#19,email#20,creation_source#21,last_session_creation_time#22,opted_in_to_mailing_list#23,enabled_for_marketing_drip#24,org_id#25,invited_by_user_id#26])\n            :     +- Relation [object_id#17,creation_time#18,name#19,email#20,creation_source#21,last_session_creation_time#22,opted_in_to_mailing_list#23,enabled_for_marketing_drip#24,org_id#25,invited_by_user_id#26] csv\n            +- SubqueryAlias adopted_users\n               +- View (`adopted_users`, [user_id#55])\n                  +- Project [user_id#55]\n                     +- Filter (sum_days#65L <= cast(7 as bigint))\n                        +- SubqueryAlias __auto_generated_subquery_name\n                           +- Project [user_id#55, sum_days#65L]\n                              +- Project [user_id#55, diff_days#60, time_stamp#54, sum_days#65L, sum_days#65L]\n                                 +- Window [sum(diff_days#60) windowspecdefinition(user_id#55, time_stamp#54 ASC NULLS FIRST, specifiedwindowframe(RowFrame, currentrow$(), 2)) AS sum_days#65L], [user_id#55], [time_stamp#54 ASC NULLS FIRST]\n                                    +- Project [user_id#55, diff_days#60, time_stamp#54]\n                                       +- SubqueryAlias diff_days\n                                          +- View (`diff_days`, [user_id#55,time_stamp#54,diff_days#60])\n                                             +- Project [user_id#55, time_stamp#54, diff_days#60]\n                                                +- Project [user_id#55, time_stamp#54, _we0#61, datediff(cast(_we0#61 as date), cast(time_stamp#54 as date)) AS diff_days#60]\n                                                   +- Window [lead(time_stamp#54, 1, null) windowspecdefinition(user_id#55, time_stamp#54 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS _we0#61], [user_id#55], [time_stamp#54 ASC NULLS FIRST]\n                                                      +- Project [user_id#55, time_stamp#54]\n                                                         +- SubqueryAlias engagement\n                                                            +- View (`engagement`, [time_stamp#54,user_id#55,visited#56])\n                                                               +- Relation [time_stamp#54,user_id#55,visited#56] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m assembler \u001b[38;5;241m=\u001b[39m VectorAssembler(inputCols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreation_source_encoded\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_session_creation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m                                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopted_in_to_mailing_list\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled_for_marketing_drip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m                                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvited_by_user_id\u001b[39m\u001b[38;5;124m\"\u001b[39m], outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_adopted\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 15\u001b[0m df_selected_features \u001b[38;5;241m=\u001b[39m \u001b[43mdf_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:3036\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2991\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   2992\u001b[0m     \u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   2993\u001b[0m \n\u001b[1;32m   2994\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3034\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3036\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `features` cannot be resolved. Did you mean one of the following? [`is_adopted`, `users`.`name`, `users`.`email`, `users`.`org_id`, `users`.`object_id`].;\n'Project ['features, is_adopted#83]\n+- Project [object_id#17, creation_time#18, name#19, email#20, creation_source#21, last_session_creation_time#22, opted_in_to_mailing_list#23, enabled_for_marketing_drip#24, org_id#25, invited_by_user_id#26, is_adopted#83, creation_source_index#219, UDF(cast(creation_source_index#219 as double), 0) AS creation_source_encoded#239]\n   +- Project [object_id#17, creation_time#18, name#19, email#20, creation_source#21, last_session_creation_time#22, opted_in_to_mailing_list#23, enabled_for_marketing_drip#24, org_id#25, invited_by_user_id#26, is_adopted#83, UDF(cast(creation_source#21 as string)) AS creation_source_index#219]\n      +- Project [object_id#17, creation_time#18, name#19, email#20, creation_source#21, last_session_creation_time#22, opted_in_to_mailing_list#23, enabled_for_marketing_drip#24, org_id#25, invited_by_user_id#26, CASE WHEN isnotnull(user_id#55) THEN true ELSE false END AS is_adopted#83]\n         +- Join LeftOuter, (object_id#17 = user_id#55)\n            :- SubqueryAlias users\n            :  +- View (`users`, [object_id#17,creation_time#18,name#19,email#20,creation_source#21,last_session_creation_time#22,opted_in_to_mailing_list#23,enabled_for_marketing_drip#24,org_id#25,invited_by_user_id#26])\n            :     +- Relation [object_id#17,creation_time#18,name#19,email#20,creation_source#21,last_session_creation_time#22,opted_in_to_mailing_list#23,enabled_for_marketing_drip#24,org_id#25,invited_by_user_id#26] csv\n            +- SubqueryAlias adopted_users\n               +- View (`adopted_users`, [user_id#55])\n                  +- Project [user_id#55]\n                     +- Filter (sum_days#65L <= cast(7 as bigint))\n                        +- SubqueryAlias __auto_generated_subquery_name\n                           +- Project [user_id#55, sum_days#65L]\n                              +- Project [user_id#55, diff_days#60, time_stamp#54, sum_days#65L, sum_days#65L]\n                                 +- Window [sum(diff_days#60) windowspecdefinition(user_id#55, time_stamp#54 ASC NULLS FIRST, specifiedwindowframe(RowFrame, currentrow$(), 2)) AS sum_days#65L], [user_id#55], [time_stamp#54 ASC NULLS FIRST]\n                                    +- Project [user_id#55, diff_days#60, time_stamp#54]\n                                       +- SubqueryAlias diff_days\n                                          +- View (`diff_days`, [user_id#55,time_stamp#54,diff_days#60])\n                                             +- Project [user_id#55, time_stamp#54, diff_days#60]\n                                                +- Project [user_id#55, time_stamp#54, _we0#61, datediff(cast(_we0#61 as date), cast(time_stamp#54 as date)) AS diff_days#60]\n                                                   +- Window [lead(time_stamp#54, 1, null) windowspecdefinition(user_id#55, time_stamp#54 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS _we0#61], [user_id#55], [time_stamp#54 ASC NULLS FIRST]\n                                                      +- Project [user_id#55, time_stamp#54]\n                                                         +- SubqueryAlias engagement\n                                                            +- View (`engagement`, [time_stamp#54,user_id#55,visited#56])\n                                                               +- Relation [time_stamp#54,user_id#55,visited#56] csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "string_indexer = StringIndexer(inputCol=\"creation_source\", outputCol=\"creation_source_index\")\n",
    "encoder = OneHotEncoder(inputCol=\"creation_source_index\", outputCol=\"creation_source_encoded\")\n",
    "assembler = VectorAssembler(inputCols=[\"object_id\", \"creation_source_encoded\", \"last_session_creation_time\",\n",
    "                                       \"opted_in_to_mailing_list\", \"enabled_for_marketing_drip\", \"org_id\",\n",
    "                                       \"invited_by_user_id\"], outputCol=\"features\")\n",
    "\n",
    "\n",
    "selected_features = [\"features\", \"is_adopted\"]\n",
    "df_selected_features = df_features.select(selected_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba7f7cd",
   "metadata": {},
   "source": [
    "The main goal was to to create a view that had an is_adopted column and this would be the main feature I would be looking at. As you can see, this failed, and this is because for some reason although the column is being created Pandas data frame is not reading the column. Nor is the PySpark dataframe machine learning tools. My query building was thinking that I could find the difference in the dates from engagement, and use that to create a view to query even further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
